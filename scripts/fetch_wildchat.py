"""
Download allenai/WildChat-1M from HuggingFace and extract benign
multi-turn conversations for L4 eval + single-turn benign for L3/L1.

Source: https://huggingface.co/datasets/allenai/WildChat-1M
838K real user conversations. Each turn has a per-turn `toxic` boolean.
We filter for conversations where ALL turns are non-toxic and there are
3+ user turns.

Outputs:
  - multiturn_wildchat_benign.yaml  (L4 eval: full messages[])
  - opensource_wildchat_benign.yaml  (L3/L1 eval: first user message)
"""

from datasets import load_dataset
import yaml
import sys
import random

MAX_MULTITURN = 10000
MAX_SINGLETURN = 2000
MIN_USER_TURNS = 3


def extract_conversation(row):
    """Extract messages and check toxicity from a WildChat row."""
    conv = row.get("conversation", [])
    if not conv:
        return None, False

    messages = []
    all_safe = True

    for turn in conv:
        role = turn.get("role", "")
        content = (turn.get("content") or "").strip()
        toxic = turn.get("toxic", False)

        if toxic:
            all_safe = False
            break

        if role in ("user", "assistant") and content:
            messages.append({"role": role, "content": content})

    return messages, all_safe


def main():
    print("Loading allenai/WildChat-1M...", file=sys.stderr)
    print("(this is a large dataset, download may take a few minutes)", file=sys.stderr)
    ds = load_dataset("allenai/WildChat-1M", split="train")
    print(f"Total rows: {len(ds)}", file=sys.stderr)

    # Shuffle indices for uniform sampling
    indices = list(range(len(ds)))
    random.seed(42)
    random.shuffle(indices)

    multiturn_cases = []
    singleturn_cases = []

    for count, idx in enumerate(indices):
        if (len(multiturn_cases) >= MAX_MULTITURN and
                len(singleturn_cases) >= MAX_SINGLETURN):
            break

        if count % 50000 == 0:
            print(f"  scanned {count}/{len(ds)}, mt={len(multiturn_cases)} st={len(singleturn_cases)}",
                  file=sys.stderr)

        row = ds[idx]
        messages, all_safe = extract_conversation(row)
        if not messages or not all_safe:
            continue

        user_turns = [m for m in messages if m["role"] == "user"]

        if len(user_turns) >= MIN_USER_TURNS and len(multiturn_cases) < MAX_MULTITURN:
            multiturn_cases.append({
                "id": f"wc-mt-{len(multiturn_cases):04d}",
                "layer": "l4",
                "label": "benign",
                "description": f"WildChat benign ({len(user_turns)} user turns)",
                "messages": messages,
            })

        if user_turns and len(singleturn_cases) < MAX_SINGLETURN:
            first_msg = user_turns[0]["content"]
            if len(first_msg) > 10:
                singleturn_cases.append({
                    "id": f"wc-st-{len(singleturn_cases):04d}",
                    "layer": "l3_inbound",
                    "label": "benign",
                    "description": "WildChat benign user message",
                    "content": first_msg,
                })

    print(f"\nMulti-turn benign: {len(multiturn_cases)}", file=sys.stderr)
    print(f"Single-turn benign: {len(singleturn_cases)}", file=sys.stderr)

    for cases, filename, header in [
        (multiturn_cases, "multiturn_wildchat_benign.yaml",
         "# allenai/WildChat-1M — benign multi-turn conversations\n"
         "# Source: https://huggingface.co/datasets/allenai/WildChat-1M\n"
         f"# Sampled {len(multiturn_cases)} conversations with {MIN_USER_TURNS}+ user turns\n"
         "# All turns verified non-toxic via per-turn toxic=false flag\n"
         "# Auto-generated by scripts/fetch_wildchat.py"),
        (singleturn_cases, "opensource_wildchat_benign.yaml",
         "# allenai/WildChat-1M — benign single-turn user messages\n"
         "# Source: https://huggingface.co/datasets/allenai/WildChat-1M\n"
         f"# Sampled {len(singleturn_cases)} first user messages from non-toxic conversations\n"
         "# Auto-generated by scripts/fetch_wildchat.py"),
    ]:
        path = f"schema/eval/{filename}"
        with open(path, "w", encoding="utf-8") as f:
            f.write(header + "\n\n")
            yaml.dump(cases, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        print(f"  wrote {len(cases)} cases to {path}", file=sys.stderr)


if __name__ == "__main__":
    main()
