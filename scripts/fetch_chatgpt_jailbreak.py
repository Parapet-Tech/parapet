# Copyright 2026 The Parapet Project
# SPDX-License-Identifier: Apache-2.0

"""
Download rubend18/ChatGPT-Jailbreak-Prompts from HuggingFace
and convert to parapet eval YAML format.

Source: https://huggingface.co/datasets/rubend18/ChatGPT-Jailbreak-Prompts
Jailbreak prompts collected from the wild. All malicious.
Used by ProtectAI for prompt injection detection training.
"""

from datasets import load_dataset
import yaml
import sys


def main():
    print("Loading rubend18/ChatGPT-Jailbreak-Prompts...", file=sys.stderr)
    ds = load_dataset("rubend18/ChatGPT-Jailbreak-Prompts", split="train")
    print(f"Total rows: {len(ds)}", file=sys.stderr)

    cases = []
    seen = set()
    for i, row in enumerate(ds):
        text = (row.get("Prompt") or row.get("prompt") or "").strip()
        if not text or text in seen:
            continue
        seen.add(text)
        name = (row.get("Name") or row.get("name") or "unknown")[:60]
        cases.append({
            "id": f"chatgpt-jb-{i:04d}",
            "layer": "l1",
            "label": "malicious",
            "description": f"ChatGPT-Jailbreak: {name}",
            "content": text,
        })

    print(f"Attack cases (deduplicated): {len(cases)}", file=sys.stderr)
    path = "schema/eval/opensource_chatgpt_jailbreak_attacks.yaml"
    with open(path, "w", encoding="utf-8") as f:
        f.write(
            "# rubend18/ChatGPT-Jailbreak-Prompts â€” jailbreak prompts (all malicious)\n"
            "# Source: https://huggingface.co/datasets/rubend18/ChatGPT-Jailbreak-Prompts\n"
            "# Auto-generated by scripts/fetch_chatgpt_jailbreak.py\n\n"
        )
        yaml.dump(cases, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
    print(f"  wrote {len(cases)} cases to {path}", file=sys.stderr)


if __name__ == "__main__":
    main()
