# Copyright 2026 The Parapet Project
# SPDX-License-Identifier: Apache-2.0

"""
Download allenai/wildguardmix from HuggingFace
and convert to parapet eval YAML format.

Source: https://huggingface.co/datasets/allenai/wildguardmix
Paper:  NeurIPS 2024 (Datasets & Benchmarks)
License: Apache 2.0

GATED DATASET: Requires HF_TOKEN environment variable or `huggingface-cli login`.

WildGuardMix is a safety classification dataset with prompt harmfulness labels.
We extract prompts labeled as "prompt_injection" or similar for attacks,
and vanilla/benign prompts for benign cases.
"""

from datasets import load_dataset
import yaml
import sys
import os
import random

MAX_ATTACKS = 2000
MAX_BENIGN = 2000
SEED = 42


def main():
    token = os.environ.get("HF_TOKEN", "")
    if not token:
        from huggingface_hub import HfFolder
        token = HfFolder.get_token()
    if not token:
        print("ERROR: wildguardmix is gated. Set HF_TOKEN or run `huggingface-cli login`.", file=sys.stderr)
        sys.exit(1)

    print("Loading allenai/wildguardmix (wildguardtrain)...", file=sys.stderr)
    ds = load_dataset("allenai/wildguardmix", "wildguardtrain", split="train", token=token)

    print(f"Total rows: {len(ds)}", file=sys.stderr)
    print(f"Columns: {ds.column_names}", file=sys.stderr)

    # Show a few samples to understand the schema
    for i in range(min(3, len(ds))):
        row = ds[i]
        print(f"\nSample {i}:", file=sys.stderr)
        for k, v in row.items():
            val = str(v)[:120]
            print(f"  {k}: {val}", file=sys.stderr)

    # Determine which field has the prompt and which has the label
    # Expected: prompt, response, prompt_harm_label, response_harm_label, harm_category
    prompt_field = None
    for candidate in ["prompt", "instruction", "text", "input"]:
        if candidate in ds.column_names:
            prompt_field = candidate
            break

    label_field = None
    for candidate in ["prompt_harm_label", "is_injection", "label", "harm_category"]:
        if candidate in ds.column_names:
            label_field = candidate
            break

    if not prompt_field:
        print(f"ERROR: Could not find prompt field. Columns: {ds.column_names}", file=sys.stderr)
        sys.exit(1)

    print(f"\nUsing prompt_field={prompt_field}, label_field={label_field}", file=sys.stderr)

    # Gather unique label values
    if label_field:
        labels = set()
        for i in range(min(1000, len(ds))):
            labels.add(str(ds[i].get(label_field, "")))
        print(f"Label values (first 1000): {labels}", file=sys.stderr)

    # Classify rows
    rng = random.Random(SEED)
    indices = list(range(len(ds)))
    rng.shuffle(indices)

    attacks = []
    benign = []
    seen = set()

    for idx in indices:
        if len(attacks) >= MAX_ATTACKS and len(benign) >= MAX_BENIGN:
            break
        row = ds[idx]
        text = str(row.get(prompt_field) or "").strip()
        if not text or len(text) < 10:
            continue
        if text in seen:
            continue
        seen.add(text)

        label_val = str(row.get(label_field, "")).lower() if label_field else ""

        # Determine if attack or benign based on label
        is_attack = label_val in ("harmful", "yes", "1", "true", "injection", "prompt_injection")

        if is_attack and len(attacks) < MAX_ATTACKS:
            attacks.append({
                "id": f"wildguardmix-atk-{len(attacks):04d}",
                "layer": "l1",
                "label": "malicious",
                "description": f"WildGuardMix label={label_val}",
                "content": text[:4000],
            })
        elif not is_attack and len(benign) < MAX_BENIGN:
            benign.append({
                "id": f"wildguardmix-ben-{len(benign):04d}",
                "layer": "l1",
                "label": "benign",
                "description": f"WildGuardMix label={label_val}",
                "content": text[:4000],
            })

    print(f"\nCollected: {len(attacks)} attacks, {len(benign)} benign", file=sys.stderr)

    header = (
        "# allenai/wildguardmix â€” safety classification dataset\n"
        "# Source: https://huggingface.co/datasets/allenai/wildguardmix\n"
        "# License: Apache 2.0\n"
        "# Auto-generated by scripts/fetch_wildguardmix.py\n"
    )

    if attacks:
        path = "schema/eval/opensource_wildguardmix_attacks.yaml"
        with open(path, "w", encoding="utf-8") as f:
            f.write(header + f"# {len(attacks)} attack cases\n\n")
            yaml.dump(attacks, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        print(f"Wrote {len(attacks)} cases to {path}", file=sys.stderr)

    if benign:
        path = "schema/eval/opensource_wildguardmix_benign.yaml"
        with open(path, "w", encoding="utf-8") as f:
            f.write(header + f"# {len(benign)} benign cases\n\n")
            yaml.dump(benign, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        print(f"Wrote {len(benign)} cases to {path}", file=sys.stderr)

    print("Done.", file=sys.stderr)


if __name__ == "__main__":
    main()
