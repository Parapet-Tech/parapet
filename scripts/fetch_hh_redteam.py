# Copyright 2026 The Parapet Project
# SPDX-License-Identifier: Apache-2.0

"""
Download Anthropic/hh-rlhf "harmless-base" subset from HuggingFace
and extract multi-turn adversarial conversations for L4 eval.

Source: https://huggingface.co/datasets/Anthropic/hh-rlhf
The "harmless-base" subset contains red-team attempts where humans
try to elicit harmful responses. Format is transcript strings
("\\n\\nHuman: ...\\n\\nAssistant: ...") that we parse into messages[].

NOTE: Only harmless-base is used — helpful-base contains benign conversations.

Filters for 3+ human turns to get multi-turn conversations.
Samples up to MAX_CASES to keep eval tractable.
"""

from datasets import load_dataset
import yaml
import sys
import random

MAX_CASES = 500
MIN_HUMAN_TURNS = 3


def parse_transcript(transcript):
    """Parse '\\n\\nHuman: ...\\n\\nAssistant: ...' into messages list."""
    messages = []
    if not transcript:
        return messages

    parts = transcript.split("\n\nHuman: ")
    for part in parts:
        if not part.strip():
            continue

        sub = part.split("\n\nAssistant: ", 1)

        human_text = sub[0].strip()
        if human_text:
            messages.append({"role": "user", "content": human_text})

        if len(sub) > 1:
            assistant_text = sub[1].strip()
            if assistant_text:
                messages.append({"role": "assistant", "content": assistant_text})

    return messages


def main():
    print("Loading Anthropic/hh-rlhf (harmless-base)...", file=sys.stderr)
    ds = load_dataset("Anthropic/hh-rlhf", data_dir="harmless-base", split="train")
    print(f"Total rows: {len(ds)}", file=sys.stderr)

    # Collect all multi-turn conversations
    candidates = []
    for i, row in enumerate(ds):
        transcript = row.get("chosen", "")
        messages = parse_transcript(transcript)

        human_turns = [m for m in messages if m["role"] == "user"]
        if len(human_turns) < MIN_HUMAN_TURNS:
            continue

        candidates.append({
            "messages": messages,
            "num_turns": len(human_turns),
        })

    print(f"Conversations with {MIN_HUMAN_TURNS}+ human turns: {len(candidates)}", file=sys.stderr)

    # Sample
    random.seed(42)
    random.shuffle(candidates)
    selected = candidates[:MAX_CASES]

    cases = []
    for i, cand in enumerate(selected):
        cases.append({
            "id": f"hh-rlhf-{i:04d}",
            "layer": "l4",
            "label": "malicious",
            "description": f"Anthropic hh-rlhf adversarial ({cand['num_turns']} turns)",
            "messages": cand["messages"],
        })

    print(f"Sampled cases: {len(cases)}", file=sys.stderr)
    path = "schema/eval/opensource_hh_redteam_attacks.yaml"
    with open(path, "w", encoding="utf-8") as f:
        f.write(
            "# Anthropic/hh-rlhf harmless-base — multi-turn red-team conversations\n"
            "# Source: https://huggingface.co/datasets/Anthropic/hh-rlhf\n"
            f"# Sampled {len(cases)} conversations with {MIN_HUMAN_TURNS}+ human turns\n"
            "# from harmless-base subset (red-team attempts). Parsed from transcript format.\n"
            "# Auto-generated by scripts/fetch_hh_redteam.py\n\n"
        )
        yaml.dump(cases, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
    print(f"  wrote {len(cases)} cases to {path}", file=sys.stderr)


if __name__ == "__main__":
    main()
